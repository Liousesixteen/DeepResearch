"""
è”ç½‘æœç´¢æ¨¡å‹å®ç°æ¨¡å—
åŒ…å«å„ç§AIæ¨¡å‹çš„è”ç½‘æœç´¢è°ƒç”¨é€»è¾‘
"""

import openai
import time
import threading
from abc import ABC, abstractmethod
from typing import Optional, Tuple, Any, Dict


class BaseSearchModel(ABC):
    """æœç´¢æ¨¡å‹åŸºç±»"""
    
    def __init__(self, model_name: str, api_configs: Dict[str, str]):
        self.model_name = model_name
        self.api_configs = api_configs
        
        # APIé…ç½® - ä½¿ç”¨æ­£ç¡®çš„é”®å
        self.primary_api = {
            "base": "https://yunwu.ai/v1",
            "key": api_configs.get("primary_key", "")
        }
        self.backup_api = {
            "base": "https://openkey.cloud/v1", 
            "key": api_configs.get("backup_key", "")
        }
        
        # è¾“å‡ºåŒæ­¥ç›¸å…³å±æ€§
        self.output_lock = None
        self.model_display_name = None
        self.is_parallel_mode = False
    
    def _setup_api_config(self, use_primary: bool = True):
        """è®¾ç½®APIé…ç½®"""
        if use_primary:
            openai.api_base = self.primary_api["base"]
            openai.api_key = self.primary_api["key"]
        else:
            openai.api_base = self.backup_api["base"]
            openai.api_key = self.backup_api["key"]
    
    def _process_streaming_response(self, completion, suppress_thinking: bool = False) -> Tuple[str, Any]:
        """å¤„ç†æµå¼å“åº”"""
        msg = ""
        thinking_content = ""
        final_content = ""
        in_thinking = False
        
        # æ˜¾ç¤ºå¼€å§‹å›ç­”çš„æ ‡è¯†
        if self.is_parallel_mode:
            self._safe_print(f"\n[{self.model_display_name}] å¼€å§‹å›ç­”ï¼š")
        else:
            print(f"\nğŸš€ {self.model_name} å¼€å§‹å›ç­”ï¼š")
        
        try:
            for chunk in completion:
                # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿chunkæœ‰choiceså±æ€§ä¸”ä¸ä¸ºç©º
                if hasattr(chunk, 'choices') and chunk.choices and len(chunk.choices) > 0:
                    choice = chunk.choices[0]
                    if hasattr(choice, 'delta') and choice.delta:
                        raw = choice.delta.get('content', '')
                        if raw:
                            msg += raw
                            text = raw
                            printable_total = ""
                            # æ˜¾å¼è§£æ <think> æ ‡ç­¾ï¼Œæ”¯æŒåŒä¸€chunkå†…å¤šæ®µ
                            while text:
                                start_idx = text.find("<think>")
                                end_idx = text.find("</think>")
                                if start_idx == -1 and end_idx == -1:
                                    # æ— æ ‡ç­¾ï¼Œæ•´æ®µå±äºå½“å‰çŠ¶æ€
                                    segment = text
                                    text = ""
                                else:
                                    # å–æœ€è¿‘çš„æ ‡ç­¾ä½ç½®
                                    cut_idx = min([i for i in [start_idx, end_idx] if i != -1])
                                    segment = text[:cut_idx]
                                    text = text[cut_idx:]
                                # å…ˆå¤„ç†å½“å‰æ®µçš„å¯æ‰“å°æ€§
                                if segment:
                                    if in_thinking and suppress_thinking:
                                        pass  # ä¸¢å¼ƒæ€ç»´æ®µå†…å®¹
                                    else:
                                        final_content += segment
                                        printable_total += segment
                                # å¤„ç†æ ‡ç­¾å¹¶æ¨è¿›çŠ¶æ€
                                if text.startswith("<think>"):
                                    in_thinking = True
                                    thinking_content += "<think>"
                                    text = text[len("<think>"):]
                                elif text.startswith("</think>"):
                                    thinking_content += "</think>"
                                    in_thinking = False
                                    text = text[len("</think>"):]
                            if printable_total:
                                print(printable_total, end="", flush=True)
                # å¦‚æœchunkæ²¡æœ‰choicesï¼Œå¯èƒ½æ˜¯ä¸‡ç äº‘APIçš„ç‰¹æ®Šæ ¼å¼
                elif hasattr(chunk, 'content'):
                    raw = chunk.content
                    if raw:
                        msg += raw
                        text = raw
                        printable_total = ""
                        while text:
                            start_idx = text.find("<think>")
                            end_idx = text.find("</think>")
                            if start_idx == -1 and end_idx == -1:
                                segment = text
                                text = ""
                            else:
                                cut_idx = min([i for i in [start_idx, end_idx] if i != -1])
                                segment = text[:cut_idx]
                                text = text[cut_idx:]
                            if segment:
                                if in_thinking and suppress_thinking:
                                    pass
                                else:
                                    final_content += segment
                                    printable_total += segment
                            if text.startswith("<think>"):
                                in_thinking = True
                                thinking_content += "<think>"
                                text = text[len("<think>"):]
                            elif text.startswith("</think>"):
                                thinking_content += "</think>"
                                in_thinking = False
                                text = text[len("</think>"):]
                        if printable_total:
                            print(printable_total, end="", flush=True)
        except Exception as e:
            error_msg = f"\nâš ï¸ å¤„ç†æµå¼å“åº”æ—¶å‡ºé”™: {e}"
            if self.is_parallel_mode:
                self._safe_print(error_msg)
            else:
                print(error_msg)
            # å¦‚æœæµå¼å¤„ç†å¤±è´¥ï¼Œå°è¯•ä»completionä¸­æå–å†…å®¹
            try:
                if hasattr(completion, 'choices') and completion.choices:
                    msg = completion.choices[0].message.get('content', '')
                elif hasattr(completion, 'content'):
                    msg = completion.content
            except:
                pass
        
        # æµå¼è¾“å‡ºç»“æŸåæ·»åŠ æ¢è¡Œ
        print()  # æ·»åŠ æ¢è¡Œ
        
        # ä¸é¢å¤–æ‰“å°æ¢è¡Œï¼Œä¿æŒè¾“å‡ºç”±ä¸Šå±‚ç»Ÿä¸€æ§åˆ¶
        
        # å¦‚æœå¯ç”¨äº†thinkingæŠ‘åˆ¶ï¼Œè¿”å›æœ€ç»ˆå†…å®¹ï¼›å¦åˆ™è¿”å›å®Œæ•´å†…å®¹
        if suppress_thinking and final_content:
            return final_content, completion
        return msg, completion
    
    def set_output_context(self, output_lock: threading.Lock, model_display_name: str):
        """è®¾ç½®è¾“å‡ºä¸Šä¸‹æ–‡ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œæ—¶çš„è¾“å‡ºåŒæ­¥"""
        self.output_lock = output_lock
        self.model_display_name = model_display_name
        self.is_parallel_mode = True
    
    def _safe_print(self, content: str, end: str = "", flush: bool = True):
        """å®‰å…¨çš„æ‰“å°æ–¹æ³•ï¼Œæ”¯æŒå¹¶è¡Œè¾“å‡ºåŒæ­¥"""
        if self.is_parallel_mode and self.output_lock:
            with self.output_lock:
                print(f"[{self.model_display_name}] {content}", end=end, flush=flush)
        else:
            print(content, end=end, flush=flush)
    
    def _safe_print_status(self, content: str, end: str = "\n", flush: bool = True):
        """å®‰å…¨çš„æ‰“å°çŠ¶æ€ä¿¡æ¯ï¼Œç”¨äºAPIè°ƒç”¨çŠ¶æ€ç­‰"""
        if self.is_parallel_mode and self.output_lock:
            with self.output_lock:
                print(f"[{self.model_display_name}] {content}", end=end, flush=flush)
        else:
            print(content, end=end, flush=flush)
    
    def _is_thinking_content(self, content: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºthinkingå†…å®¹"""
        thinking_indicators = [
            "è®©æˆ‘æ€è€ƒä¸€ä¸‹", "è®©æˆ‘æƒ³æƒ³", "æˆ‘æ¥åˆ†æä¸€ä¸‹", "è®©æˆ‘ç ”ç©¶ä¸€ä¸‹",
            "è®©æˆ‘æœç´¢ä¸€ä¸‹", "æˆ‘æ¥æŸ¥æ‰¾", "è®©æˆ‘æ¢ç´¢", "æˆ‘æ¥ç ”ç©¶",
            "Let me think", "Let me search", "Let me analyze", "Let me explore",
            "I'll think about", "I'll search for", "I'll analyze", "I'll explore",
            "æ€è€ƒä¸­", "åˆ†æä¸­", "æœç´¢ä¸­", "ç ”ç©¶ä¸­",
            "Thinking", "Analyzing", "Searching", "Researching"
        ]
        
        content_lower = content.lower()
        return any(indicator.lower() in content_lower for indicator in thinking_indicators)
    
    def _extract_final_answer(self, full_response: str) -> str:
        """ä»å®Œæ•´å“åº”ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        # ç§»é™¤thinkingç›¸å…³å†…å®¹
        thinking_patterns = [
            r"è®©æˆ‘æ€è€ƒä¸€ä¸‹.*?",
            r"è®©æˆ‘æƒ³æƒ³.*?",
            r"æˆ‘æ¥åˆ†æä¸€ä¸‹.*?",
            r"è®©æˆ‘ç ”ç©¶ä¸€ä¸‹.*?",
            r"Let me think.*?",
            r"Let me search.*?",
            r"Let me analyze.*?",
            r"æ€è€ƒä¸­.*?",
            r"åˆ†æä¸­.*?",
            r"æœç´¢ä¸­.*?"
        ]
        
        import re
        result = full_response
        for pattern in thinking_patterns:
            result = re.sub(pattern, "", result, flags=re.DOTALL | re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
        result = re.sub(r'\n\s*\n', '\n\n', result)
        result = result.strip()
        
        return result if result else full_response
    
    def _handle_api_error(self, err: Exception, attempt: int, max_retries: int) -> Tuple[bool, str]:
        """ç»Ÿä¸€çš„é”™è¯¯å¤„ç†æ–¹æ³•"""
        error_msg = str(err)
        error_type = type(err).__name__
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
        if 'RateLimitError' in error_type or 'è´Ÿè½½å·²é¥±å’Œ' in error_msg or 'overloaded' in error_msg.lower():
            if attempt == 0:
                return True, "åˆ‡æ¢åˆ°å¤‡ç”¨API"
            else:
                return False, "æ‰€æœ‰APIæä¾›å•†éƒ½é‡åˆ°è´Ÿè½½é—®é¢˜"
        
        # å…¶ä»–é”™è¯¯ç±»å‹
        if attempt < max_retries:
            return True, "æ™®é€šé”™è¯¯ï¼Œå‡†å¤‡é‡è¯•"
        else:
            return False, f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {error_msg}"
    
    def search_with_retry(self, query: str, streaming: bool = True, model_display_name: str = None, 
                         temperature: float = None, max_tokens: int = None, request_timeout: int = None,
                         suppress_thinking: bool = False) -> Tuple[Optional[str], Any]:
        """ç»Ÿä¸€çš„æœç´¢é‡è¯•æ–¹æ³•ï¼Œæ”¯æŒè‡ªå®šä¹‰å‚æ•°"""
        if model_display_name is None:
            model_display_name = self.model_name
            
        # ä½¿ç”¨é»˜è®¤å‚æ•°æˆ–è‡ªå®šä¹‰å‚æ•°
        temp = temperature if temperature is not None else 0.7
        tokens = max_tokens if max_tokens is not None else 3000
        timeout = request_timeout if request_timeout is not None else 150
            
        # ä¸»APIé‡è¯•3æ¬¡
        for attempt in range(3):
            try:
                self._setup_api_config(use_primary=True)
                
                start_time = time.time()
                self._safe_print_status(f"å°è¯•è°ƒç”¨ {model_display_name} API (å°è¯• {attempt + 1}/3, ä¸»API)")
                self._safe_print_status(f"API Base: {openai.api_base}")
                self._safe_print_status(f"Model: {self.model_name}")
                # print(f"å‚æ•°: temperature={temp}, max_tokens={tokens}")

                completion = openai.ChatCompletion.create(
                    model=self.model_name,
                    messages=[{'role': 'user', 'content': query}],
                    stream=streaming,
                    temperature=temp,
                    max_tokens=tokens,
                    request_timeout=timeout
                )
                msg = None

                if streaming:
                    msg, completion = self._process_streaming_response(completion, suppress_thinking)
                else:
                    if not completion.choices:
                        raise ValueError("No choices returned from API.")
                    msg = completion.choices[0].message['content']
                    if suppress_thinking and msg:
                        msg = self._extract_final_answer(msg)

                if msg:
                    end_time = time.time()
                    self._safe_print_status(f"è€—æ—¶: {end_time - start_time:.2f} ç§’")
                    # åœ¨å¹¶è¡Œæ¨¡å¼ä¸‹ä¸é‡å¤æ‰“å°å›ç­”å†…å®¹ï¼Œå› ä¸ºæµå¼è¾“å‡ºå·²ç»æ˜¾ç¤ºäº†
                    if not self.is_parallel_mode:
                        self._safe_print_status(f"å›ç­”å†…å®¹: {msg}")
                    return msg, completion
                else:
                    raise ValueError("No valid response received from API")

            except Exception as err:
                self._safe_print_status(f"API Error: {str(err)}")
                self._safe_print_status(f"Error type: {type(err).__name__}")
                
                if attempt < 2:  # å‰ä¸¤æ¬¡é‡è¯•
                    self._safe_print_status(f"ğŸ”„ å‡†å¤‡é‡è¯•... (å°è¯• {attempt + 2}/3)")
                    time.sleep(2 ** (attempt + 2))
                    continue
                else:  # ç¬¬3æ¬¡å¤±è´¥ååˆ‡æ¢åˆ°å¤‡ç”¨API
                    self._safe_print_status("âš ï¸ æ£€æµ‹åˆ°ä¸»APIæš‚æ—¶ä¸å¯ç”¨")
                    self._safe_print_status("ğŸ”„ å°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨ API...")
                    break
        
        # å¤‡ç”¨APIé‡è¯•3æ¬¡
        self._safe_print_status("ğŸ”„ å·²åˆ‡æ¢åˆ°å¤‡ç”¨APIï¼Œå‡†å¤‡é‡è¯•...")
        for attempt in range(3):
            try:
                self._setup_api_config(use_primary=False)
                
                start_time = time.time()
                self._safe_print_status(f"å°è¯•è°ƒç”¨ {model_display_name} API (å°è¯• {attempt + 1}/3, å¤‡ç”¨API)")
                self._safe_print_status(f"API Base: {openai.api_base}")
                self._safe_print_status(f"Model: {self.model_name}")
                # print(f"å‚æ•°: temperature={temp}, max_tokens={tokens}")

                completion = openai.ChatCompletion.create(
                    model=self.model_name,
                    messages=[{'role': 'user', 'content': query}],
                    stream=streaming,
                    temperature=temp,
                    max_tokens=tokens,
                    request_timeout=timeout
                )
                msg = None

                if streaming:
                    msg, completion = self._process_streaming_response(completion, suppress_thinking)
                else:
                    if not completion.choices:
                        raise ValueError("No choices returned from API.")
                    msg = completion.choices[0].message['content']
                    if suppress_thinking and msg:
                        msg = self._extract_final_answer(msg)

                if msg:
                    end_time = time.time()
                    self._safe_print_status(f"è€—æ—¶: {end_time - start_time:.2f} ç§’")
                    # åœ¨å¹¶è¡Œæ¨¡å¼ä¸‹ä¸é‡å¤æ‰“å°å›ç­”å†…å®¹ï¼Œå› ä¸ºæµå¼è¾“å‡ºå·²ç»æ˜¾ç¤ºäº†
                    if not self.is_parallel_mode:
                        self._safe_print_status(f"å›ç­”å†…å®¹: {msg}")
                    return msg, completion
                else:
                    raise ValueError("No valid response received from API")

            except Exception as err:
                self._safe_print_status(f"API Error: {str(err)}")
                self._safe_print_status(f"Error type: {type(err).__name__}")
                
                if attempt < 2:  # å‰ä¸¤æ¬¡é‡è¯•
                    self._safe_print_status(f"ğŸ”„ å‡†å¤‡é‡è¯•... (å°è¯• {attempt + 2}/3)")
                    time.sleep(2 ** (attempt + 2))
                    continue
                else:  # ç¬¬3æ¬¡å¤±è´¥
                    self._safe_print_status("âš ï¸ æ£€æµ‹åˆ°å¤‡ç”¨APIæš‚æ—¶ä¸å¯ç”¨")
                    break
        
        self._safe_print_status("âŒ æ‰€æœ‰ API éƒ½æš‚æ—¶ä¸å¯ç”¨")
        self._safe_print_status(f"âŒ {model_display_name} è°ƒç”¨å¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆå“åº”")
        self._safe_print_status("å¯èƒ½çš„åŸå› ï¼š")
        self._safe_print_status("1. æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜")
        self._safe_print_status("2. æ¨¡å‹æš‚æ—¶ä¸å¯ç”¨")
        self._safe_print_status("3. ç½‘ç»œè¿æ¥é—®é¢˜")
        self._safe_print_status("4. API Key é—®é¢˜")
        return None, None



    @abstractmethod
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """æœç´¢æ–¹æ³• - å­ç±»å¿…é¡»å®ç°"""
        pass


class GoogleDeepResearch(BaseSearchModel):
    """Google Deep Research æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("gemini-2.5-flash-deepsearch", api_configs)
        # è‡ªå®šä¹‰å‚æ•°
        self.temperature = 0.7
        self.max_tokens = 3000
        self.request_timeout = 150
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ Google Deep Research æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "Google Deep Research",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class GoogleDeepResearchPro(BaseSearchModel):
    """Google Deep Research Pro æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("gemini-2.5-pro-deepsearch", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - Proç‰ˆæœ¬ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
        self.temperature = 0.7
        self.max_tokens = 4000
        self.request_timeout = 180
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ Google Deep Research Pro æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "Google Deep Research Pro",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class GPTSearch(BaseSearchModel):
    """GPT Search æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("gpt-4o-search-preview-2025-03-11", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - GPT Searchä¼˜åŒ–å‚æ•°
        self.temperature = 0.7
        self.max_tokens = 3500
        self.request_timeout = 120
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ GPT Search æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "GPT Search",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class DeepSeekSearch(BaseSearchModel):
    """DeepSeek Search æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("deepseek-r1-searching", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - DeepSeekæœç´¢ä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 3200
        self.request_timeout = 150
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ DeepSeek Search æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "DeepSeek Search",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class GrokDeepSearch(BaseSearchModel):
    """Grok Deep Search æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("grok-3-deepsearch", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - Grokæœç´¢ä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 2800
        self.request_timeout = 120
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ Grok Deep Search æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "Grok Deep Search",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class KimiSearch(BaseSearchModel):
    """Kimi Search æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("kimi-k2-0711-preview-search", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - Kimiæœç´¢ä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 3000
        self.request_timeout = 150
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ Kimi Search æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "Kimi Search",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class GPT4Gizmo(BaseSearchModel):
    """GPT-4 Gizmo æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("gpt-4-gizmo-*", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - Gizmoä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 2500
        self.request_timeout = 100
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ GPT-4 Gizmo æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "GPT-4 Gizmo",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class DeepSeekV3(BaseSearchModel):
    """DeepSeek V3 æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("deepseek-v3-250324", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - DeepSeek V3ä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 3500
        self.request_timeout = 150
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ DeepSeek V3 æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "DeepSeek V3",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class GPT4All(BaseSearchModel):
    """GPT-4 All æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("gpt-4-all", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - GPT-4 Allä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 3000
        self.request_timeout = 120
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ GPT-4 All æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "GPT-4 All",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class GPT4oAll(BaseSearchModel):
    """GPT-4o All æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("gpt-4o-all", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - GPT-4o Allä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 3200
        self.request_timeout = 100
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ GPT-4o All æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "GPT-4o All",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class Gemini25FlashAll(BaseSearchModel):
    """Gemini 2.5 Flash All æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("gemini-2.5-flash-all", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - Gemini Flashä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 2800
        self.request_timeout = 120
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ Gemini 2.5 Flash All æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "Gemini 2.5 Flash All",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class Gemini25ProAll(BaseSearchModel):
    """Gemini 2.5 Pro All æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("gemini-2.5-pro-all", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - Gemini Proä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 4000
        self.request_timeout = 180
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ Gemini 2.5 Pro All æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "Gemini 2.5 Pro All",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class O3DeepResearch20250626(BaseSearchModel):
    """O3 Deep Research 2025-06-26 æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("o3-deep-research-2025-06-26", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - O3 Deep Researchä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 3500
        self.request_timeout = 150
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ O3 Deep Research 2025-06-26 æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "O3 Deep Research 2025-06-26",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class O3DeepResearch(BaseSearchModel):
    """O3 Deep Research æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("o3-deep-research", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - O3 Deep Researchä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 3500
        self.request_timeout = 150
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ O3 Deep Research æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "O3 Deep Research",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class O4MiniDeepResearch20250626(BaseSearchModel):
    """O4 Mini Deep Research 2025-06-26 æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("o4-mini-deep-research-2025-06-26", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - O4 Miniä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 2500
        self.request_timeout = 120
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ O4 Mini Deep Research 2025-06-26 æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "O4 Mini Deep Research 2025-06-26",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class O4MiniDeepResearch(BaseSearchModel):
    """O4 Mini Deep Research æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("o4-mini-deep-research", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - O4 Miniä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 2500
        self.request_timeout = 120
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ O4 Mini Deep Research æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "O4 Mini Deep Research",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class HunyuanT1(BaseSearchModel):
    """Hunyuan T1 æ¨¡å‹"""
    
    def __init__(self, api_configs: Dict[str, str]):
        super().__init__("hunyuan-t1-latest", api_configs)
        # è‡ªå®šä¹‰å‚æ•° - Hunyuan T1ä¼˜åŒ–
        self.temperature = 0.7
        self.max_tokens = 3000
        self.request_timeout = 150
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ Hunyuan T1 æ¨¡å‹"""
        return self.search_with_retry(
            query, streaming, "Hunyuan T1",
            self.temperature, self.max_tokens, self.request_timeout,
            suppress_thinking
        )


class HunyuanT1Latest(BaseSearchModel):
    """Hunyuan T1 Latest æ¨¡å‹ - ä½¿ç”¨ä¸‡ç äº‘APIï¼Œæ”¯æŒæœç´¢åŠŸèƒ½"""
    
    def __init__(self, api_configs: Dict[str, str]):
        # ä½¿ç”¨ç‰¹æ®Šçš„APIé…ç½®
        self.wcode_api_config = {
            "base": "https://wcode.net/api/gpt/v1",
            "key": "sk-1402.tw0EkGK5AeD783OPqyt6DeTtFvVaE9NOE5OXtfDLoR5o28oW"
        }
        super().__init__("hunyuan-t1-latest", api_configs)
    
    def _setup_wcode_api_config(self):
        """è®¾ç½®ä¸‡ç äº‘APIé…ç½®"""
        openai.api_base = self.wcode_api_config["base"]
        openai.api_key = self.wcode_api_config["key"]
    
    def search(self, query: str, streaming: bool = True, max_retries: int = 2, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """è°ƒç”¨ Hunyuan T1 Latest æ¨¡å‹ï¼ˆæ”¯æŒæœç´¢ï¼‰"""
        # ä½¿ç”¨ä¸‡ç äº‘APIé…ç½®
        self._setup_wcode_api_config()
        
        # ç›´æ¥ä½¿ç”¨åŸºç±»çš„search_with_retryæ–¹æ³•ï¼Œä½†é‡å†™APIè°ƒç”¨éƒ¨åˆ†
        return self._search_with_wcode_api_simple(query, streaming, "Hunyuan T1 Latest", suppress_thinking)
    
    def _search_with_wcode_api_simple(self, query: str, streaming: bool = True, model_display_name: str = None, suppress_thinking: bool = True) -> Tuple[Optional[str], Any]:
        """ä½¿ç”¨ä¸‡ç äº‘APIçš„ç®€åŒ–æœç´¢æ–¹æ³•"""
        if model_display_name is None:
            model_display_name = self.model_name
            
        try:
            start_time = time.time()
            print(f"è°ƒç”¨ {model_display_name} API (ä¸‡ç äº‘)")
            print(f"API Base: {openai.api_base}")
            print(f"Model: {self.model_name}")
            print("ğŸ” æœç´¢åŠŸèƒ½å·²å¼€å¯")

            # ä½¿ç”¨æœç´¢å¢å¼ºå‚æ•°
            completion = openai.ChatCompletion.create(
                model=self.model_name,
                messages=[{'role': 'user', 'content': query}],
                stream=streaming,
                temperature=0.7,
                max_tokens=3000,
                request_timeout=150,
                force_search_enhancement=True  # å¼€å¯æœç´¢å¢å¼º
            )
            msg = None

            if streaming:
                msg, completion = self._process_streaming_response(completion, suppress_thinking)
            else:
                if not completion.choices:
                    raise ValueError("No choices returned from API.")
                msg = completion.choices[0].message['content']
                if suppress_thinking and msg:
                    msg = self._extract_final_answer(msg)

            if msg:
                end_time = time.time()
                print(f"è€—æ—¶: {end_time - start_time:.2f} ç§’    {model_display_name} (æœç´¢å¢å¼º) output: {msg}")
                return msg, completion
            else:
                raise ValueError("No valid response received from API")

        except Exception as err:
            print(f"API Error: {str(err)}")
            print(f"Error type: {type(err).__name__}")
            print(f"âŒ {model_display_name} è°ƒç”¨å¤±è´¥")
            return None, None
